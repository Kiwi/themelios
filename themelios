#!/usr/bin/env bash
# MIT License

# Copyright 2018 Adam Schaefers sch@efers.org

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

usage() {
    cat << EOF
    usage:
        themelios configuration.sh git-remote [branch]

use -h or --help for more information.
EOF
    exit
}

help() {
    cat << EOF
    usage:
        themelios configuration.sh git-remote [branch]

for detailed examples and instructions please visit the github page:

    https://github.com/a-schaefers/themelios

essentially, feed themelios a git repository url that contains a file which has the following configuration variables:

# Themelios configuration.sh example

# DISK PREPARATION SETTINGS #

use_sgdisk_clear="true"    # use sgdisk --clear
use_wipefs_all="true"      # use wipefs --all
use_zero_disks="false"     # use dd if=/dev/zero ...

# ZFS POOL SETTINGS #

zfs_pool_name="zroot"
zfs_pool_type="mirror"     # use "" for single, or "mirror", "raidz1", etc.

# Note: using /dev/disk/by-id is also preferable.
zfs_pool_disks=("/dev/sda" "/dev/sdb")

# Datasets to be set with com.sun:auto-snapshot=true.
zfs_auto_snapshot=("$zfs_pool_name/HOME" "$zfs_pool_name/ROOT")

# If true, mount /nix outside of the / (root) dataset.
# Recommended true for now due to https://github.com/a-schaefers/themelios/issues/1
zfs_dataset_slashnix_no_root="true"

# Use atime?
zfs_use_atime="false"            # (recommended "false" for ssd.)

zfs_make_swap="false"            # creates a swap zvol (Not recommended in zfs-land.)
zfs_swap_size="4G"

# If set, themelios will source them if the files exist alongside configuration.sh
zfs_pool_overlay_file=""         # override zpool_create()
zfs_dataset_overlay_file=""      # override datasets_create()
postinstall_overlay_file=""      # run arbritrary code after nixos-install and before umount /mnt.

# NIX_OS BOOTSTRAP SETTINGS #

# Your top-level configuration.nix file to be bootstrapped-- (use the relative path from the project_root.)
# For example, to bootstrap project_root/hosts/vm-example/default.nix
nix_top_level_configuration="hosts/vm-example"

# Directory name of to clone your git-remote in "/" (root). Do not use slashes.
# This is intended to be the directory to operate the nix installation from.
# For example, here is mine! https://github.com/a-schaefers/nix-config
nix_repo_name="nix-config"

# Optionally inserted as "nixos-install --root /mnt $nix_install_opts"
nix_install_opts=""

# Creates /etc/nixos/zfs-configuration.nix with sensible settings.
nix_zfs_configuration_enabled="true"

# Enable "extra" options [below] in addition to zfs_configuration?
nix_zfs_configuration_extra_enabled="true"

# Enables periodic scrubbing of ZFS pools.
nix_zfs_extra_auto_scrub="true"

# Enable the (OpenSolaris-compatible) ZFS auto-snapshotting service.
nix_zfs_extra_auto_snapshot_enabled="true"
nix_zfs_extra_auto_snapshot_frequent="8"   # take a snapshot every 15 minutes and keep 8 in rotation
nix_zfs_extra_auto_snapshot_hourly="0"
nix_zfs_extra_auto_snapshot_daily="7"      # take a daily snapshot and keep 7 in rotation
nix_zfs_extra_auto_snapshot_weekly="0"
nix_zfs_extra_auto_snapshot_monthly="0"

# Use NixOs automatic garbage collection?
nix_zfs_extra_gc_automatic="true"
nix_zfs_extra_gc_dates="weekly"
nix_zfs_extra_gc_options="--delete-older-than 30d"

# Clean /tmp automatically on boot.
nix_zfs_extra_clean_tmp_dir="true"
EOF
    exit
}

function die() {
    echo "$1"
    exit 1
}

start_over() {
    rm -rf /tmp/cloned_remote
    umount /mnt/home
    umount /mnt/tmp
    umount /mnt
    swapoff /dev/zvol/"$POOL"/swap/swap0
    zpool destroy "$POOL"
}

initial_warning() {
    echo "WARNING: The following script intends to replace all of your disk(s) \
contents with a zfs-on-root NixOS installation and bootstrap your configuration.nix."

    read -p "Ready? (Y or N) " -n 1 -r
    [[ ! $REPLY =~ ^[Yy]$ ]] && die "Aborted."
}

fail_warning() {
    read -p "The previous command failed. Continue running script? (Y or N) " -n 1 -r
    [[ ! $REPLY =~ ^[Yy]$ ]] && die "Aborted."
}

uefi_or_legacy() {
    # detect if this is a uefi installation.
    [[ -d "/sys/firmware/efi/efivars" ]] && uefi_install="1"

    # some initial translation for whether or not the script was provided disks with sd* or /dev/disk/by-id/*
    [[ $(echo ${zfs_pool_disks[1]} | grep "/dev/sd") ]] && use_uefi_sdX="1"
    if [[ $use_uefi_sdX ]]
    then
        efi_part="2"
        zpool_partition="3"
    else
        efi_part="-part2"
        zpool_partition="-part3"
    fi
}

switch_if_needed() {
    nixos-rebuild switch || fail_warning
}

bootstrap_zfs() {
    sed -i '/imports/a boot.supportedFilesystems = [ \"zfs\" ];' \
        /etc/nixos/configuration.nix
    needs_switch="1"
}

bootstrap_git() {
    sed -i '/imports/a environment.systemPackages = with pkgs; [ git ];' \
        /etc/nixos/configuration.nix
    needs_switch="1"
}

get_custom_nixcfg() {
    # clone the declared repo from git_remote ($2)
    # optional git_branch ($3) may be used additionally for all users who need to build from a non-master branch.

    # github users' https remote will be switched to ssh remote automatically after initial clone.
    # github users also may use the "shorthand" username/repo-name

    # FIXME grep isn't accurate enough and probably wrong tool for this job.
    github_shorthand=$(echo "$git_remote" | grep -v ".git")
    github_url=$(echo "$git_remote" | grep "github.com")

    checkout_branch() {
        echo "checking out $github_sshremote"
        cd /tmp/cloned_remote || return 1
        git checkout "$git_branch"
    }

    switch_github_remotes() {
        echo "switching remote from https to $github_sshremote for user convenience..."
        cd /tmp/cloned_remote || return 1
        git remote set-url origin "$github_sshremote"
    }

    github_clone_and_switch() {
        echo "cloning repo via $github_httpsremote"
        github_httpsremote=https://github.com/$github_user/$github_repo
        github_sshremote=git@github.com:$github_user/$github_repo
        git clone "$github_httpsremote" /tmp/cloned_remote || fail_warning
        switch_github_remotes
        [[ $git_branch ]] && checkout_branch
    }

    if [[ $github_url ]]
    then
        # FIXME cut will stop working if github ever changes
        github_user=$(echo "$git_remote" | grep github | cut -d '/' -f 4)
        github_repo=$(echo "$git_remote" | grep github | cut -d '/' -f 5)
        github_clone_and_switch

    elif [[ $github_shorthand ]]
    then
        github_user=$(echo "$git_remote" | cut -d '/' -f 1)
        github_repo=$(echo "$git_remote" | cut -d '/' -f 2)
        github_clone_and_switch

    else
        echo "cloning repo via $git_remote"
        git clone "$git_remote" /tmp/cloned_remote || fail_warning
        [[ $git_branch ]] && checkout_branch
    fi

    # to find the users configuration.sh file:
    # first we simply use "./the/path/to/$1" if the user provides it.
    # second if $1 has no slashes, then we search for $1 guessing it maybe a unique filename in the project.
    # third we search again for a unique dirname, but only if the filename search turns up nothing,
    # then we append the literal "/configuration.sh" convention to the unique dirname, making assumption it must be this.
    # finally, in all cases no matter what we use, we also need to setup optional overlay files which must be beside configuration.sh, wherever that may be.
    find_configuration_sh() {
        # if given a unique filename, use the filename to source the config variables.
        # find via 'cd' and '.' gives relative path to file that is needed.
        cd /tmp/cloned_remote || return 1
        find . -type f -name "$config_dot_sh" | grep "." && \
            config_dot_sh=$(find . -type f -name "$config_dot_sh") && \
            [ ! -e "$config_dot_sh" ] && die "error: themelios cannot figure out which $config_dot_sh to use. in this case, feed themelios ./the/entire/path to the configuration.sh file you are trying to use with startover=1 pool=$zfs_pool_name and try again."

        # if given a unique dirname, use unique_dirname/configuration.sh convention to source the variables.
        find . -type d -name "$config_dot_sh" | grep "." && \
            config_dot_sh=$(echo $(find . -type d -name "$config_dot_sh")/configuration.sh)
    }
    # if not given a path with slashes to a configuration.sh file, then use find.
    echo "$config_dot_sh" | grep "/" || find_configuration_sh

    source "$config_dot_sh" || die "$config_dot_sh file not found."

    # prepare optional overlay locations based on where the users configuration.sh is.
    zfs_pool_overlay_file=${config_dot_sh%/*}/$zfs_pool_overlay_file
    zfs_dataset_overlay_file=${config_dot_sh%/*}/$zfs_dataset_overlay_file
    postinstall_overlay_file=${config_dot_sh%/*}/$postinstall_overlay_file
}

disk_prep() {
    sgdisk_clear() {
        for disk_id in "${zfs_pool_disks[@]}"
        do
            echo "clearing disk with sgdisk..."
            sgdisk --clear "$disk_id" || fail_warning
        done
    }
    [[ $use_sgdisk_clear == "true" ]] && sgdisk_clear

    wipefs_all() {
        for disk_id in "${zfs_pool_disks[@]}"
        do
            echo "wiping disk signatures with wipefs..."
            wipefs -fa "$disk_id" || fail_warning
        done
    }
    [[ $use_wipefs_all == "true" ]] && wipefs_all

    dd_zero() {
        for disk_id in "${zfs_pool_disks[@]}"
        do
            echo "writing zeros to ${disk_id}..."
            dd if=/dev/zero of="$disk_id" bs=1M oflag=direct status=progress &
        done
        wait
    }
    [[ $use_zero_disks == "true" ]] && dd_zero
}

zpool_create() {
    # convert option from nix true/false style to proper zfs on/off.
    if [[ $zfs_use_atime == "true" ]] ; then zfs_use_atime="on" ; else zfs_use_atime="off" ; fi

    # join each array item together into one variable, separated by spaces.
    join_by() { local IFS="$1"; shift; echo "$*"; }
    zfs_pool_disks_joined=( "${zfs_pool_disks[@]/%/$zpool_partition}" )
    zfs_pool_disks_joined=$(join_by ' ' "${zfs_pool_disks_joined[@]}")

    echo "creating zpool..."
    zpool create -f \
          -o ashift=12 \
          -O compression=lz4 \
          -O atime=$zfs_use_atime \
          -O relatime=on \
          -O normalization=formD \
          -O xattr=sa \
          -m none \
          -R /mnt \
          $zfs_pool_name \
          $zfs_pool_type \
          $zfs_pool_disks_joined || fail_warning

    # https://github.com/NixOS/nixpkgs/issues/16954
    zfs set acltype=posixacl "$zfs_pool_name"
}

uefi_disk_part() {
    # make 1mb bios boot, 1G efi/boot, remaining partition for the zpool
    for disk_id in "${zfs_pool_disks[@]}"
    do
        sgdisk -og "$disk_id"
        sgdisk -n 1:2048:4095 -c 1:"BIOS Boot Partition" -t 1:ef02 "$disk_id"
        sgdisk -n 2:4096:+1G -c 2:"EFI System Partition" -t 2:ef00 "$disk_id"
        ENDSECTOR=`sgdisk -E "$disk_id"`
        sgdisk -n 3:2101248:$ENDSECTOR -c 3:"Linux ZPOOL Partition" -t 3:8300 "$disk_id"
        sgdisk -p "$disk_id"
        partx -u "$disk_id"
    done
}

uefi_mkfs() {
    # make fat32 for /boot on all disks
    for disk_id in "${zfs_pool_disks[@]}"
    do
        # https://wiki.archlinux.org/index.php/EFI_system_partition#Format_the_partition
        mkfs.vfat -F32 "$disk_id""$efi_part" || mkfs.fat -s2 -F32 "$disk_id""$efi_part"
    done

    mkdir /mnt/boot
    mount "${zfs_pool_disks[1]}""$efi_part" /mnt/boot
}

create_bios_boot_partitions() {
    for disk_id in "${zfs_pool_disks[@]}"
    do
        echo "creating bios boot partition..."
        sgdisk -a1 -n2:48:2047 -t2:ef02 -c2:"bios boot partition" "$disk_id" || fail_warning
        partx -u "$disk_id"
    done
}

datasets_create() {
    echo "Creating and mounting datasets in /mnt..."
    # / (root) datasets
    zfs create -o mountpoint=none -o canmount=off -o sync=always "$zfs_pool_name"/ROOT
    zfs create -o mountpoint=legacy -o canmount=on "$zfs_pool_name"/ROOT/nixos
    mount -t zfs "$zfs_pool_name"/ROOT/nixos /mnt
    zpool set bootfs="$zfs_pool_name"/ROOT/nixos "$zfs_pool_name"

    # for UEFI systems - create/mount the EFI partition to /boot with a fat32 fs.
    [[ $uefi_install ]] && uefi_mkfs

    slash_nix_outside_root() {
        zfs create -o mountpoint=none -o canmount=off -o sync=always "$zfs_pool_name"/NIX
        zfs create -o mountpoint=legacy -o canmount=on "$zfs_pool_name"/NIX/nix
        mkdir /mnt/nix
        mount -t zfs "$zfs_pool_name"/NIX/nix /mnt/nix
    }
    [[ $zfs_dataset_slashnix_no_root == "true" ]] && slash_nix_outside_root

    mkdir -p /mnt/{home,tmp}

    # /home datasets
    zfs create -o mountpoint=none -o canmount=off "$zfs_pool_name"/HOME
    zfs create -o mountpoint=legacy -o canmount=on "$zfs_pool_name"/HOME/home
    mount -t zfs "$zfs_pool_name"/HOME/home /mnt/home

    # /tmp datasets
    zfs create -o mountpoint=none -o canmount=off "$zfs_pool_name"/TMP
    zfs create -o mountpoint=legacy -o canmount=on -o sync=disabled "$zfs_pool_name"/TMP/tmp
    mount -t zfs "$zfs_pool_name"/TMP/tmp /mnt/tmp

    # swap option
    create_zswap() {
        echo "Creating ZSWAP volume and turning on swap..."
        zfs create \
            -o primarycache=metadata \
            -o secondarycache=metadata \
            -o compression=zle \
            -o sync=always \
            -o logbias=throughput \
            -o com.sun:auto-snapshot=false \
            "$zfs_pool_name"/SWAP

        zfs create \
            -V "$zfs_swap_size" \
            -b "$(getconf PAGESIZE)" \
            "$zfs_pool_name"/SWAP/swap0

        mkswap -f /dev/zvol/"$zfs_pool_name"/SWAP/swap0
        swapon /dev/zvol/"$zfs_pool_name"/SWAP/swap0
    }
    [[ $zfs_make_swap == "true" ]] && create_zswap

    zfs_auto_snapshot() {
        for dataset in "${zfs_auto_snapshot[@]}"
        do
            echo "Setting property com.sun:auto-snapshot=true to ${dataset}..."
            zfs set com.sun:auto-snapshot=true "$dataset"
        done
    }
    zfs_auto_snapshot
}

bootstrap_nixcfg() {
    echo "moving repo from /tmp to /mnt/$nix_repo_name..."
    cp -rp /tmp/cloned_remote /mnt/"$nix_repo_name"

    # this is for generating our ./hardware-configuration.nix files.
    # ./configuration.nix will be overwritten shortly hereafter.
    echo "executing nixos-generate-config --root /mnt"
    nixos-generate-config --root /mnt || fail_warning

    # give a chance for advanced user who is hacking the script in the case of
    # installation into an already created pool to pass his already existing hostid to script. :)
    [[ ! $zfs_host_id ]] && zfs_host_id="random"
    hostid_generate() {
        echo "generating random hostid..."
        zfs_host_id="$(head -c4 /dev/urandom | od -A none -t x4 | cut -d ' ' -f 2)"
        echo "$zfs_host_id"
    }
    [[ $zfs_host_id == "random" ]] && hostid_generate

    # strip potential prefixed './'
    nix_top_level_configuration=$(echo "$nix_top_level_configuration" | sed 's|^./||')
    # strip potential trailing '/'
    nix_top_level_configuration=${nix_top_level_configuration%/}

    # create /mnt/etc/nixos/configuration.nix and import user's top_level_nixfile.
    # maybe create /mnt/etc/nixos/zfs-configuration.nix
    # maybe append extra configured options to zfs-configuration.nix
    # maybe populate configuration.nix networking.host_id = \"$zfs_host_id\";
    if [[ $nix_zfs_configuration_enabled == "true" ]]
    then
        maybe_create_zfs_configuration="./zfs-configuration.nix"
    else
        maybe_append_host_id="networking.host_id = \"$zfs_host_id\";"
    fi
    cat << EOF > /mnt/etc/nixos/configuration.nix
{ ... }:
{ imports = [
../../$nix_repo_name/$nix_top_level_configuration
./hardware-configuration.nix
$maybe_create_zfs_configuration
];
$maybe_append_host_id
}
EOF

    [[ $uefi_install ]] && maybe_grub_zfsSupport="boot.supportedFilesystems = [ \"zfs\" ];"

    [[ $nix_zfs_configuration_enabled == "true" ]] && cat << EOF > /mnt/etc/nixos/zfs-configuration.nix
{ ... }:
{ imports = [];

# Configure grub with efi or zfs support if needed
$maybe_grub_zfsSupport

# tmp
$(if [[ $uefi_install ]]
then
cat <<- UEFI
boot.loader = {
  efi = {
    canTouchEfiVariables = true;
    efiSysMountPoint = "/boot"; # use the same mount point here.
  };
  grub = {
     efiSupport = true;
     #efiInstallAsRemovable = true; # in case canTouchEfiVariables doesn't work for your system
     device = "nodev";
  };
};

UEFI
else
cat <<- LEGACY

boot.loader.grub.enable = true;
boot.loader.grub.version = 2;
boot.loader.grub.devices = [
$(for disk_id in "${zfs_pool_disks[@]}"
do
echo "\"$disk_id\""
done)
];

LEGACY
fi)

# The 32-bit host id of the machine, formatted as 8 hexadecimal characters.
# You should try to make this id unique among your machines.
networking.hostId = "$zfs_host_id";

# noop, the recommended elevator with zfs.
# shell_on_fail allows to force import manually in the case of zfs import failure.
boot.kernelParams = [ "elevator=noop" "boot.shell_on_fail" ];

# Grub on zfs has been known to have a hard time finding kernels with really/long/dir/paths.
# Copy the kernels to /boot and avoid the issue.
boot.loader.grub.copyKernels = true;

# Ensure some safeguards are active that zfs uses to protect zfs pools.
boot.zfs.forceImportAll = false;
boot.zfs.forceImportRoot = false;

$([[ $nix_zfs_configuration_extra_enabled == "true" ]] && cat <<- EXTRA
# Enables periodic scrubbing of ZFS pools.
services.zfs.autoScrub.enable = $nix_zfs_extra_auto_scrub;

# Enable the (OpenSolaris-compatible) ZFS auto-snapshotting service.
services.zfs.autoSnapshot = {
enable = $nix_zfs_extra_auto_snapshot_enabled;
frequent = $nix_zfs_extra_auto_snapshot_frequent;
hourly = $nix_zfs_extra_auto_snapshot_hourly;
daily = $nix_zfs_extra_auto_snapshot_daily;
weekly = $nix_zfs_extra_auto_snapshot_weekly;
monthly = $nix_zfs_extra_auto_snapshot_monthly;
};

# Use gc.automatic with autoSnapshot to keep disk space under control.
nix.gc.automatic = $nix_zfs_extra_gc_automatic;
nix.gc.dates = "$nix_zfs_extra_gc_dates";
nix.gc.options = "$nix_zfs_extra_gc_options";

# Clean /tmp automatically on boot.
boot.cleanTmpDir = $nix_zfs_extra_clean_tmp_dir;
EXTRA
)
}
EOF

    # give user a retry option with --show-trace to have a chance to fix imports on another tty. :)
    nixos-install-show-trace() {
        nixos-install --root /mnt --show-trace $nix_install_opts || nixos-install_fail_retry
    }

    nixos-install_fail_retry() {
        echo "themelios hint: check /mnt/etc/nixos/configuration.nix and your other files in /mnt/$nix_repo_name before trying again."
        echo "themelios hint: make sure you are using relative path imports for all of your .nix files."
        read -p "nixos-install --root /mnt failed, retry? will add --show-trace (y or n) " -n 1 -r
        if [[ $REPLY =~ ^[Yy]$ ]]
        then
            nixos-install-show-trace
        else
            die "the only steps remaining after nixos-install --root /mnt should be unmounting /mnt and exporting the pool :) good luck."
        fi
    }

    install() {
        echo "executing nixos-install --root /mnt"
        nixos-install --root /mnt $nix_install_opts || nixos-install_fail_retry
    }

    install
}

thank_you() {
    cat << EOF
    nnnnnnnn        nnnnnnnniiiiiiiiiixxxxxxx       xxxxxxx
    n:::::::n       n::::::ni::::::::ix:::::x       x:::::x
    n::::::::n      n::::::ni::::::::ix:::::x       x:::::x
    n:::::::::n     n::::::nii::::::iix::::::x     x::::::x
    n::::::::::n    n::::::n  i::::i  xxx:::::x   x:::::xxx
    n:::::::::::n   n::::::n  i::::i     x:::::x x:::::x
    n:::::::n::::n  n::::::n  i::::i      x:::::x:::::x
    n::::::n n::::n n::::::n  i::::i       x:::::::::x
    n::::::n  n::::n:::::::n  i::::i       x:::::::::x
    n::::::n   n:::::::::::n  i::::i      x:::::x:::::x
    n::::::n    n::::::::::n  i::::i     x:::::x x:::::x
    n::::::n     n:::::::::n  i::::i  xxx:::::x   x:::::xxx
    n::::::n      n::::::::nii::::::iix::::::x     x::::::x
    n::::::n       n:::::::ni::::::::ix:::::x       x:::::x
    n::::::n        n::::::ni::::::::ix:::::x       x:::::x
    nnnnnnnn         nnnnnnniiiiiiiiiixxxxxxx       xxxxxxx
EOF
    # give the user one last chance to do things
    [[ -f $postinstall_overlay_file ]] && source $postinstall_overlay_file

    umount_export() {
        [[ $zfs_make_swap == "true" ]] && swapoff /dev/zvol/"$zfs_pool_name"/swap/swap0

        echo "unmounting /mnt"
        umount /mnt/home
        umount /mnt/tmp
        umount /mnt

        echo "exporting $zfs_pool_name"
        zpool export "$zfs_pool_name"

        read -p "finished. reboot now? (y or n) " -n 1 -r
        [[ $REPLY =~ ^[Yy]$ ]] && reboot

    }

    # pass NOUMOUNT=1 to not unmount /mnt or export the pool after install automatically
    [[ ! ${NOUMOUNT} ]] && umount_export

    exit
}

# start executing code !

# rerun script with "STARTOVER=1 POOL=zroot themelios foo bar" to attempt a fresh start.
[[ ${STARTOVER} ]] && start_over

# arguments
[[ $1 == "-h" ]] && help
[[ $1 == "--help" ]] && help
[[ $# -lt 2 ]] && usage
[[ $# -gt 3 ]] && usage
config_dot_sh=$1
git_remote=$2
git_branch=$3

# warn user of potential doom.
initial_warning

# install zfs if needed to the livedisk.
which zfs > /dev/null 2>&1 || bootstrap_zfs

# install git if needed to the livedisk.
which git > /dev/null 2>&1 || bootstrap_git

# reconfigure nix livedisk if needed.
[[ $needs_switch ]] && switch_if_needed

# clone the repo and get all of the configuration information.
get_custom_nixcfg

# check for legacy or uefi bios.
uefi_or_legacy

# use sgdisk wipefs and dd tools to cleanup old disks.
disk_prep

# if uefi partition disks for a uefi disk scheme
[[ $uefi_install ]] && uefi_disk_part

# create default zpool or maybe-use optional user overlay instead.
if [[ -f $zfs_pool_overlay_file ]]
then
    source $zfs_pool_overlay_file
else
    zpool_create
fi

[[ ! $uefi_install ]] && create_bios_boot_partitions

# create default datasets or maybe-use optional user overlay instead.
if [[ -f $zfs_dataset_overlay_file ]]
then
    source $zfs_dataset_overlay_file
else
    datasets_create
fi

# bootstrap the users custom nix configurations.
bootstrap_nixcfg

# may you have a happy hacking. :)
thank_you
